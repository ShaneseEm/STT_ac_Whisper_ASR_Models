{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":7837168,"datasetId":4594013,"databundleVersionId":7941127}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# Whisper Amharic Fine-tuning Pipeline (Final Clean Version)\n# ==============================================================================\n\n# ----------------------------\n# 0) INSTALLS & ENVIRONMENT FIXES\n# ----------------------------\nprint(\"Step 0: Installing required libraries...\")\n!pip install -q \"protobuf==3.20.3\"\n!pip install -q transformers datasets evaluate soundfile librosa accelerate jiwer\n!pip install torchcodec\n!pip install tqdm\nprint(\"Installation complete.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:52:59.225040Z","iopub.execute_input":"2025-12-05T13:52:59.225578Z","iopub.status.idle":"2025-12-05T13:54:29.636467Z","shell.execute_reply.started":"2025-12-05T13:52:59.225557Z","shell.execute_reply":"2025-12-05T13:54:29.635702Z"}},"outputs":[{"name":"stdout","text":"Step 0: Installing required libraries...\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting torchcodec\n  Downloading torchcodec-0.9.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (11 kB)\nDownloading torchcodec-0.9.0-cp311-cp311-manylinux_2_28_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torchcodec\nSuccessfully installed torchcodec-0.9.0\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nInstallation complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# ----------------------------\n# 1) IMPORTS\n# ----------------------------\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nimport librosa\nimport torch\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List\nfrom datasets import load_dataset, Dataset, DatasetDict, Audio\nfrom transformers import (\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments\n)\nfrom transformers import logging as hf_logging\nfrom evaluate import load as load_metric\n\nhf_logging.set_verbosity_error()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:54:29.638471Z","iopub.execute_input":"2025-12-05T13:54:29.638890Z","iopub.status.idle":"2025-12-05T13:54:56.803562Z","shell.execute_reply.started":"2025-12-05T13:54:29.638863Z","shell.execute_reply":"2025-12-05T13:54:56.802946Z"}},"outputs":[{"name":"stderr","text":"2025-12-05 13:54:40.208130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764942880.393223      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764942880.444872      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ----------------------------\n# 2) CONFIGURATION\n# ----------------------------\nprint(\"\\nStep 1: Setting up configuration...\")\n\n# --- DATASET PATH ---\n# These paths are confirmed to be correct.\nDATASET_BASE_PATH = \"/kaggle/input/amharic-speech-corpus\"\nDATASET_ROOT_DIR = os.path.join(DATASET_BASE_PATH, \"AMHARIC\")\nTRAIN_DATA_DIR = os.path.join(DATASET_ROOT_DIR, \"data\", \"train\")\nTEST_DATA_DIR = os.path.join(DATASET_ROOT_DIR, \"data\", \"test\")\n\n# --- MODEL & OUTPUT SETTINGS ---\nMODEL_NAME = \"openai/whisper-base\"\nOUTPUT_DIR = \"./whisper-amharic-finetuned\"\nPROCESSED_DS_DIR = \"./processed_whisper_amharic\"\n\n# --- TRAINING PARAMETERS ---\nSAMPLE_RATE = 16000\nMAX_AUDIO_SECONDS = 30.0\nBATCH_SIZE = 8\nNUM_EPOCHS = 6\nLEARNING_RATE = 1e-5\nACCUMULATION_STEPS = 2\nSEED = 42\nNUM_PROC = 4\n\n# --- LOGGING & SAVING ---\nUSE_FP16 = True\nLOGGING_STEPS = 50\nEVAL_STEPS = 500\nSAVE_STEPS = 500\n\n# Create output directories\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(PROCESSED_DS_DIR, exist_ok=True)\n\nprint(\"Configuration set. Paths verified.\")\nprint(f\"  - Train Data: {TRAIN_DATA_DIR}\")\nprint(f\"  - Test Data:  {TEST_DATA_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:54:56.804276Z","iopub.execute_input":"2025-12-05T13:54:56.804836Z","iopub.status.idle":"2025-12-05T13:54:56.811164Z","shell.execute_reply.started":"2025-12-05T13:54:56.804803Z","shell.execute_reply":"2025-12-05T13:54:56.810430Z"}},"outputs":[{"name":"stdout","text":"\nStep 1: Setting up configuration...\nConfiguration set. Paths verified.\n  - Train Data: /kaggle/input/amharic-speech-corpus/AMHARIC/data/train\n  - Test Data:  /kaggle/input/amharic-speech-corpus/AMHARIC/data/test\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ----------------------------\n# 3) UTILITY FUNCTIONS\n# ----------------------------\ndef normalize_amharic_text(text):\n    if text is None: return \"\"\n    s = str(text)\n    s = re.sub(r\"[A-Za-z]\", \"\", s)\n    s = re.sub(r\"[\\u0000-\\u007F]+\", \" \", s)\n    s = re.sub(r\"[!\\\"#\\$%&\\(\\)\\*\\+,\\-\\.\\/:;<=>\\?@\\[\\]\\\\\\^_`{\\|}~]\", \" \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef load_kaldi_data(data_dir):\n    \"\"\"Loads data from a Kaldi-style directory and returns a Pandas DataFrame.\"\"\"\n    print(f\"\\n--- Loading data from: {data_dir} ---\")\n    wav_scp_path = os.path.join(data_dir, \"wav.scp\")\n    text_path = os.path.join(data_dir, \"text\")\n\n    if not os.path.exists(wav_scp_path):\n        print(f\"ERROR: wav.scp not found at {wav_scp_path}\")\n        return pd.DataFrame()\n    if not os.path.exists(text_path):\n        print(f\"ERROR: text not found at {text_path}\")\n        return pd.DataFrame()\n\n    wav_scp = {}\n    with open(wav_scp_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split(' ', 1)\n            if len(parts) == 2:\n                file_id, wav_path = parts\n                if not os.path.isabs(wav_path):\n                    wav_path = os.path.join(data_dir, wav_path)\n                wav_scp[file_id] = wav_path\n    \n    text = {}\n    with open(text_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split(' ', 1)\n            if len(parts) == 2:\n                file_id, transcription = parts\n                text[file_id] = transcription\n    \n    print(f\"Found {len(wav_scp)} entries in wav.scp\")\n    print(f\"Found {len(text)} entries in text\")\n\n    data = []\n    for file_id, path in wav_scp.items():\n        if file_id in text:\n            data.append({'path': path, 'sentence': text[file_id]})\n            \n    print(f\"Successfully matched {len(data)} entries.\")\n    print(\"--- Data loading finished ---\")\n            \n    return pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:54:56.812121Z","iopub.execute_input":"2025-12-05T13:54:56.812540Z","iopub.status.idle":"2025-12-05T13:54:56.834602Z","shell.execute_reply.started":"2025-12-05T13:54:56.812522Z","shell.execute_reply":"2025-12-05T13:54:56.834015Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ==============================================================================\n# CELL 4: DATA LOADING AND PREPARATION (Modified to fix error)\n# ==============================================================================\n\n# --- Imports needed for this cell ---\nimport os\nimport re\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict # NO LONGER NEED Audio import here\n\n# --- Configuration variables needed for this cell ---\nDATASET_BASE_PATH = \"/kaggle/input/amharic-speech-corpus\"\nDATASET_ROOT_DIR = os.path.join(DATASET_BASE_PATH, \"AMHARIC\")\nTRAIN_DATA_DIR = os.path.join(DATASET_ROOT_DIR, \"data\", \"train\")\nTEST_DATA_DIR = os.path.join(DATASET_ROOT_DIR, \"data\", \"test\")\n# SAMPLE_RATE = 16000 # Not needed here anymore\n\n# --- Utility functions needed for this cell ---\ndef normalize_amharic_text(text):\n    if text is None: return \"\"\n    s = str(text)\n    s = re.sub(r\"[A-Za-z]\", \"\", s)\n    s = re.sub(r\"[\\u0000-\\u007F]+\", \" \", s)\n    s = re.sub(r\"[!\\\"#\\$%&\\(\\)\\*\\+,\\-\\.\\/:;<=>\\?@\\[\\]\\\\\\^_`{\\|}~]\", \" \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef load_kaldi_data(data_dir):\n    \"\"\"Loads data from a Kaldi-style directory and returns a Pandas DataFrame.\"\"\"\n    print(f\"--- Loading data from: {data_dir} ---\")\n    wav_scp_path = os.path.join(data_dir, \"wav.scp\")\n    text_path = os.path.join(data_dir, \"text\")\n    \n    wav_scp = {}\n    with open(wav_scp_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split('\\t', 1)\n            if len(parts) == 2:\n                file_id = parts[0]\n                filename = f\"{file_id}.wav\"\n                wav_path = os.path.join(data_dir, \"wav\", filename)\n                wav_scp[file_id] = wav_path\n    \n    text = {}\n    with open(text_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split(' ', 1)\n            if len(parts) == 2:\n                file_id, transcription = parts\n                text[file_id] = transcription\n    \n    print(f\"Found {len(wav_scp)} entries in wav.scp and {len(text)} in text.\")\n    data = []\n    for file_id, path in wav_scp.items():\n        if file_id in text:\n            data.append({'path': path, 'sentence': text[file_id]})\n    print(f\"Successfully matched {len(data)} entries.\")\n    return pd.DataFrame(data)\n\n# --- Main data loading logic for this cell ---\nprint(\"Step 4: Loading and preparing the dataset...\")\ntrain_df = load_kaldi_data(TRAIN_DATA_DIR)\ntest_df = load_kaldi_data(TEST_DATA_DIR)\n\nfor df in [train_df, test_df]:\n    df['sentence'] = df['sentence'].apply(normalize_amharic_text)\n    df = df[df['path'].apply(os.path.exists)]\n    df = df[df['sentence'].str.strip().astype(bool)]\n    df.reset_index(drop=True, inplace=True)\n\nval_df = test_df\nprint(f\"Final dataset sizes: Train={len(train_df)}, Validation={len(val_df)}\")\n\ndef make_hf_dataset(df):\n    # --- CHANGE: Create a simple 'audio' column with just the path ---\n    recs = [{\"audio\": p, \"sentence\": s} for p, s in zip(df['path'], df['sentence'])]\n    return Dataset.from_list(recs)\n\ntrain_ds = make_hf_dataset(train_df)\nval_ds = make_hf_dataset(val_df)\n\n# --- IMPORTANT CHANGE: REMOVE THE .cast_column() CALL ---\n# We will handle audio loading manually in the next step.\n# train_ds = train_ds.cast_column(\"audio\", Audio(sampling_rate=SAMPLE_RATE))\n# val_ds = val_ds.cast_column(\"audio\", Audio(sampling_rate=SAMPLE_RATE))\n\ndataset = DatasetDict({\"train\": train_ds, \"validation\": val_ds})\nprint(\"\\nâœ… Cell 4 finished successfully! The 'dataset' variable is ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:54:56.835345Z","iopub.execute_input":"2025-12-05T13:54:56.835565Z","iopub.status.idle":"2025-12-05T13:58:53.035673Z","shell.execute_reply.started":"2025-12-05T13:54:56.835548Z","shell.execute_reply":"2025-12-05T13:58:53.035045Z"}},"outputs":[{"name":"stdout","text":"Step 4: Loading and preparing the dataset...\n--- Loading data from: /kaggle/input/amharic-speech-corpus/AMHARIC/data/train ---\nFound 10875 entries in wav.scp and 10875 in text.\nSuccessfully matched 10875 entries.\n--- Loading data from: /kaggle/input/amharic-speech-corpus/AMHARIC/data/test ---\nFound 359 entries in wav.scp and 359 in text.\nSuccessfully matched 359 entries.\nFinal dataset sizes: Train=10875, Validation=359\n\nâœ… Cell 4 finished successfully! The 'dataset' variable is ready.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Let's inspect the first 10 lines of the wav.scp files\nTRAIN_WAV_SCP = \"/kaggle/input/amharic-speech-corpus/AMHARIC/data/train/wav.scp\"\nTEST_WAV_SCP = \"/kaggle/input/amharic-speech-corpus/AMHARIC/data/test/wav.scp\"\n\nprint(\"First 10 lines of TRAIN wav.scp:\")\n!head -10 {TRAIN_WAV_SCP}\n\nprint(\"\\n\" + \"=\"*40 + \"\\n\")\n\nprint(\"First 10 lines of TEST wav.scp:\")\n!head -10 {TEST_WAV_SCP}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:58:53.036467Z","iopub.execute_input":"2025-12-05T13:58:53.036714Z","iopub.status.idle":"2025-12-05T13:58:53.326393Z","shell.execute_reply.started":"2025-12-05T13:58:53.036695Z","shell.execute_reply":"2025-12-05T13:58:53.325593Z"}},"outputs":[{"name":"stdout","text":"First 10 lines of TRAIN wav.scp:\ntr_10000_tr097082\t/home/melese/kaldi/data/train/wav/tr_10000_tr097082.wav\ntr_10001_tr097083\t/home/melese/kaldi/data/train/wav/tr_10001_tr097083.wav\ntr_10002_tr097084\t/home/melese/kaldi/data/train/wav/tr_10002_tr097084.wav\ntr_10003_tr097085\t/home/melese/kaldi/data/train/wav/tr_10003_tr097085.wav\ntr_10004_tr097086\t/home/melese/kaldi/data/train/wav/tr_10004_tr097086.wav\ntr_10005_tr097087\t/home/melese/kaldi/data/train/wav/tr_10005_tr097087.wav\ntr_10006_tr097088\t/home/melese/kaldi/data/train/wav/tr_10006_tr097088.wav\ntr_10007_tr097089\t/home/melese/kaldi/data/train/wav/tr_10007_tr097089.wav\ntr_10008_tr097090\t/home/melese/kaldi/data/train/wav/tr_10008_tr097090.wav\ntr_10009_tr097091\t/home/melese/kaldi/data/train/wav/tr_10009_tr097091.wav\n\n========================================\n\nFirst 10 lines of TEST wav.scp:\n01_d501021\t/home/melese/kaldi/data/test/wav/01_d501021.wav\n01_d501022\t/home/melese/kaldi/data/test/wav/01_d501022.wav\n01_d501023\t/home/melese/kaldi/data/test/wav/01_d501023.wav\n01_d501024\t/home/melese/kaldi/data/test/wav/01_d501024.wav\n01_d501025\t/home/melese/kaldi/data/test/wav/01_d501025.wav\n01_d501026\t/home/melese/kaldi/data/test/wav/01_d501026.wav\n01_d501027\t/home/melese/kaldi/data/test/wav/01_d501027.wav\n01_d501028\t/home/melese/kaldi/data/test/wav/01_d501028.wav\n01_d501029\t/home/melese/kaldi/data/test/wav/01_d501029.wav\n01_d501030\t/home/melese/kaldi/data/test/wav/01_d501030.wav\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: PROCESSOR AND MODEL SETUP (Detailed Version)\n# ==============================================================================\n\n# --- Imports needed for this cell ---\nimport torch\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\n\n# --- Configuration variables needed for this cell ---\n# (These should be defined in your earlier configuration cell)\nMODEL_NAME = \"openai/whisper-base\"\n\n# --- Main model loading logic for this cell ---\nprint(\"Step 5: Loading processor and model...\")\n\n# 1. Load the processor (handles tokenization and feature extraction)\nprint(\"  -> Loading WhisperProcessor...\")\nprocessor = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"amharic\", task=\"transcribe\")\nprint(\"  -> Processor loaded successfully.\")\n\n# 2. Load the pre-trained model\nprint(\"  -> Loading WhisperForConditionalGeneration model...\")\nprint(\"     (This may take a few minutes the first time as it downloads the model weights)\")\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\nprint(\"  -> Model loaded successfully.\")\n\n# 3. Configure the model for Amharic transcription\nprint(\"  -> Configuring model for Amharic...\")\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"amharic\", task=\"transcribe\")\nmodel.gradient_checkpointing_enable() # Saves memory\nprint(\"  -> Model configured.\")\n\n# 4. Move the model to the GPU if available\nif torch.cuda.is_available():\n    print(\"  -> Moving model to GPU...\")\n    model = model.to(\"cuda\")\n    print(\"  -> Model moved to GPU.\")\nelse:\n    print(\"  -> CUDA not available, using CPU.\")\n\nprint(\"\\nâœ… Cell 5 finished successfully! Processor and model are ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:58:53.328913Z","iopub.execute_input":"2025-12-05T13:58:53.329661Z","iopub.status.idle":"2025-12-05T13:58:58.455455Z","shell.execute_reply.started":"2025-12-05T13:58:53.329632Z","shell.execute_reply":"2025-12-05T13:58:58.454749Z"}},"outputs":[{"name":"stdout","text":"Step 5: Loading processor and model...\n  -> Loading WhisperProcessor...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35710b99afe943f582b4a1d9bfdd4478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e92648cecf1a465dbccb30e3a519412e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e44b8780354de39bfea24cc386bdfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"906a4e65fb704d22a09ce6d31ba3b537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b7ec558f7b043da902ea3560a5447b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c570b3ae86ef4362890c11cb83027c7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d02ae215e847b3b04aa85f6439ae98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b3db7b07eb24ad8ab629f38e91b2160"}},"metadata":{}},{"name":"stdout","text":"  -> Processor loaded successfully.\n  -> Loading WhisperForConditionalGeneration model...\n     (This may take a few minutes the first time as it downloads the model weights)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"636271f8592c4f19bc969affb22dfc2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0505d25b2a2c4aae8e002b8c6919ce36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6264aa83714691ab787c7ff02fb356"}},"metadata":{}},{"name":"stdout","text":"  -> Model loaded successfully.\n  -> Configuring model for Amharic...\n  -> Model configured.\n  -> Moving model to GPU...\n  -> Model moved to GPU.\n\nâœ… Cell 5 finished successfully! Processor and model are ready.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6: DATA PREPROCESSING (Generator Version - Final Fix)\n# ==============================================================================\n\n# --- Imports needed for this cell ---\nimport os\nimport numpy as np\nimport soundfile as sf\nimport librosa\nfrom datasets import Dataset, DatasetDict, load_from_disk\n\n# --- Configuration and variables needed for this cell ---\nPROCESSED_DS_DIR = \"./processed_whisper_amharic\"\nMAX_AUDIO_SECONDS = 30.0\nSAMPLE_RATE = 16000\n\n# These variables should be available from running cells 4 and 5\ndataset = dataset\nprocessor = processor\n\n# --- The preprocessing function (no changes needed here) ---\ndef prepare_example(example):\n    audio_path = example[\"audio\"]\n    speech_array, sr = sf.read(audio_path)\n    if sr != SAMPLE_RATE:\n        speech_array = librosa.resample(np.asarray(speech_array, dtype=np.float32), orig_sr=sr, target_sr=SAMPLE_RATE)\n    if speech_array.ndim > 1:\n        speech_array = np.mean(speech_array, axis=1)\n    max_len = int(MAX_AUDIO_SECONDS * SAMPLE_RATE)\n    if len(speech_array) > max_len:\n        speech_array = speech_array[:max_len]\n    input_features = processor.feature_extractor(speech_array, sampling_rate=SAMPLE_RATE).input_features[0]\n    labels = processor(text=example[\"sentence\"], text_target=example[\"sentence\"]).input_ids\n    return {\"input_features\": input_features, \"labels\": labels}\n\n# --- NEW: A generator function to yield examples one by one ---\ndef gen_processed_examples(ds):\n    \"\"\"\n    This generator yields processed examples one at a time,\n    which is very memory-efficient.\n    \"\"\"\n    for example in ds:\n        yield prepare_example(example)\n\n# --- Main preprocessing logic for this cell ---\nprint(\"\\nStep 6: Preprocessing audio and text data using a generator...\")\n\nif not os.path.exists(os.path.join(PROCESSED_DS_DIR, \"dataset_dict.json\")):\n    print(\"Processing dataset for the first time. This will take a while...\")\n    \n    # Create datasets from the generators. This avoids memory overflow.\n    print(\"Creating training dataset from generator...\")\n    train_ds = Dataset.from_generator(gen_processed_examples, gen_kwargs={\"ds\": dataset[\"train\"]})\n    \n    print(\"Creating validation dataset from generator...\")\n    val_ds = Dataset.from_generator(gen_processed_examples, gen_kwargs={\"ds\": dataset[\"validation\"]})\n    \n    # Combine into a DatasetDict\n    proc_ds = DatasetDict({\"train\": train_ds, \"validation\": val_ds})\n    \n    # Save the processed dataset to disk\n    print(\"Saving processed dataset to disk...\")\n    proc_ds.save_to_disk(PROCESSED_DS_DIR)\n    print(\"Processing complete and saved to disk.\")\n    \nelse:\n    print(\"Found processed dataset on disk. Loading from cache...\")\n    proc_ds = load_from_disk(PROCESSED_DS_DIR)\n    print(\"Loading from disk complete.\")\n\nprint(\"\\nâœ… Cell 6 finished successfully! Processed data is ready for training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T13:58:58.456175Z","iopub.execute_input":"2025-12-05T13:58:58.456419Z","iopub.status.idle":"2025-12-05T14:05:03.088398Z","shell.execute_reply.started":"2025-12-05T13:58:58.456399Z","shell.execute_reply":"2025-12-05T14:05:03.087542Z"}},"outputs":[{"name":"stdout","text":"\nStep 6: Preprocessing audio and text data using a generator...\nProcessing dataset for the first time. This will take a while...\nCreating training dataset from generator...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3278666b86db4c5c9e04a274a3d07af0"}},"metadata":{}},{"name":"stdout","text":"Creating validation dataset from generator...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a16a3d7e2446cba11ff8963989cd89"}},"metadata":{}},{"name":"stdout","text":"Saving processed dataset to disk...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/21 shards):   0%|          | 0/10875 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1c6c81d08a84f999d299b5922ef1a2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/359 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eaa179676cb4adb93acf9013626d218"}},"metadata":{}},{"name":"stdout","text":"Processing complete and saved to disk.\n\nâœ… Cell 6 finished successfully! Processed data is ready for training.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ==============================================================================\n# CELL 7: DATA COLLATOR AND METRICS\n# ==============================================================================\n\n# --- Imports needed for this cell ---\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List\nfrom evaluate import load as load_metric\n\n# --- Variables needed for this cell ---\n# These should be available from running cells 4, 5, and 6\nprocessor = processor\n\n# ==============================================================================\n# PART 1: DATA COLLATOR\n# This function takes a batch of examples and pads them so they are all the same size.\n# ==============================================================================\n@dataclass\nclass DataCollatorSpeechSeq2Seq:\n    processor: WhisperProcessor\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        # Pad the input features (audio data)\n        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # Pad the labels (text data) and replace padding token id with -100\n        labels = [f[\"labels\"] for f in features]\n        labels_batch = self.processor.tokenizer.pad({\"input_ids\": labels}, return_tensors=\"pt\")\n        \n        # The -100 tells the model to ignore these tokens when calculating the loss\n        labels_batch[\"input_ids\"] = np.where(\n            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id, -100, labels_batch[\"input_ids\"]\n        )\n        \n        batch[\"labels\"] = torch.tensor(labels_batch[\"input_ids\"])\n        return batch\n\n# Create an instance of our data collator\ndata_collator = DataCollatorSpeechSeq2Seq(processor=processor)\n\n# ==============================================================================\n# PART 2: METRICS\n# This function calculates the Word Error Rate (WER) to see how well the model is doing.\n# ==============================================================================\nwer_metric = load_metric(\"wer\")\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    # Replace -100 with the pad token id so we can decode the labels\n    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n    \n    # Decode the predicted and label IDs back into text\n    decoded_preds = processor.tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Calculate the Word Error Rate\n    wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"wer\": wer}\n\nprint(\"\\nâœ… Cell 7 finished successfully! Data collator and metrics are ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:05:03.089277Z","iopub.execute_input":"2025-12-05T14:05:03.089824Z","iopub.status.idle":"2025-12-05T14:05:04.136077Z","shell.execute_reply.started":"2025-12-05T14:05:03.089804Z","shell.execute_reply":"2025-12-05T14:05:04.135450Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a8fcb3e83c445539c6b73decb12af42"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Cell 7 finished successfully! Data collator and metrics are ready.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==============================================================================\n# CELL 8: TRAINING THE MODEL (Corrected)\n# ==============================================================================\n\n# --- Imports needed for this cell ---\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n# --- Variables needed for this cell ---\n# These should be available from running cells 4, 5, 6, and 7\nproc_ds = proc_ds\nmodel = model\ndata_collator = data_collator\ncompute_metrics = compute_metrics\n\n# ==============================================================================\n# PART 1: SET UP TRAINING ARGUMENTS\n# These arguments control the entire training process.\n# ==============================================================================\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-amharic-finetuned\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    eval_steps=500,\n    save_steps=500,\n    logging_steps=50,\n    learning_rate=1e-5,\n    num_train_epochs=6,\n    weight_decay=0.01,\n    predict_with_generate=True,\n    fp16=True, # Use mixed-precision if a GPU is available\n    report_to=None, # Set to \"wandb\" if you want to use Weights & Biases\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    save_total_limit=3,\n)\n\n# ==============================================================================\n# PART 2: INITIALIZE THE TRAINER\n# The trainer orchestrates the entire training and evaluation loop.\n# ==============================================================================\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=proc_ds[\"train\"],\n    eval_dataset=proc_ds[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=processor.feature_extractor,\n    compute_metrics=compute_metrics,\n)\n\n# ==============================================================================\n# PART 3: START TRAINING!\n# This is the final command that begins the fine-tuning process.\n# ==============================================================================\nprint(\"\\nğŸš€ Starting the training process... ğŸš€\")\ntrainer.train()\n\n# ==============================================================================\n# PART 4: SAVE THE FINAL MODEL\n# After training, save the best model and the processor.\n# ==============================================================================\nprint(\"\\nâœ… Training finished! Saving the final model...\")\ntrainer.save_model(\"./whisper-amharic-finetuned\")\nprocessor.save_pretrained(\"./whisper-amharic-finetuned\")\nprint(\"Model and processor saved to './whisper-amharic-finetuned'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:18:18.163386Z","iopub.execute_input":"2025-12-05T14:18:18.164022Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/4118250548.py:44: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"\nğŸš€ Starting the training process... ğŸš€\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null}]}